{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session_4_Q3X_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6LMZdOt6o+HHFEMwGkFEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tapasML/EVAB2/blob/main/Session_4_Q3X_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtbLBAupvEvL"
      },
      "source": [
        "#                                                 **This is EMNIST**\r\n",
        "\r\n",
        "#  Different Datasets are available .\r\n",
        "# Split 'byclass' is **unbalanced** (training accuracy lesser) \r\n",
        "# compared to digits/ MNIST/ letters (**balanced** train set)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRf-spLDWGzC",
        "outputId": "4f35b8da-467b-42c2-fe6e-1d03aef90970"
      },
      "source": [
        "#import torch, tochvision packages \r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim \r\n",
        "import torchvision \r\n",
        "import torchvision.transforms as transforms  \r\n",
        "\r\n",
        "#define device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "#download the dataset in /data_emnist folder as Tensors\r\n",
        "emnist_train_set = torchvision.datasets.EMNIST(\r\n",
        "    root='./data_emnist_class'\r\n",
        "    ,train=True\r\n",
        "    #,split='byclass' #split type =byclass\r\n",
        "    ,split='digits' #split type =digits\r\n",
        "    ,download=True\r\n",
        "    ,transform=transforms.Compose([\r\n",
        "        # if split 'byclass' vertical 90 degree + horizontal flip image to dispay properly.\r\n",
        "        #lambda img: torchvision.transforms.functional.rotate(img, -90), \r\n",
        "        #lambda img: torchvision.transforms.functional.hflip(img),\r\n",
        "        transforms.ToTensor()\r\n",
        "    ])\r\n",
        ")\r\n",
        "dataset_size= len(emnist_train_set)\r\n",
        "print('dataset size = ', dataset_size) #around 700K images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset size =  240000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmS794WIv3x6"
      },
      "source": [
        "# **Build the Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "688V-MZIw2Cx"
      },
      "source": [
        "#@disclaimer: get_num_correct() function is example of plagiarism\r\n",
        "\r\n",
        "#utility method to count how many predictions match label.\r\n",
        "def get_num_correct(preds, labels):\r\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()\r\n",
        "\r\n",
        "#define the network\r\n",
        "class Emnist_Network(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__() #initialize parent       \r\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,  out_channels=10, kernel_size=3, padding=1)#input:28X28X1,  out:28X28X10, kernel:3X3, RF: 3\r\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, padding=1)#input:28X28X10, out:28X28X10, kernel:3X3, RF: 5\r\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  #input:28X28X10, out:14X14X10, kernel:2X2, stride=2, RF: 10\r\n",
        "\r\n",
        "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1)#input:14X14X10, out:14X14X20, kernel:3X3, RF: 12\r\n",
        "        self.conv4 = nn.Conv2d(in_channels=20, out_channels=20, kernel_size=3, padding=1)#input:14X14X20, out:14X14X20, kernel:3X3, RF: 14\r\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  #input:14X14X20, out:7X7X20, kernel:2X2, stride=2, RF: 28\r\n",
        "\r\n",
        "        self.conv5 = nn.Conv2d(in_channels=20, out_channels=30, kernel_size=3) #input:7X7X20, out:5X5X30, kernel:3X3, RF: 30\r\n",
        "\r\n",
        "        # for split by class\r\n",
        "        #self.conv6 = nn.Conv2d(in_channels=30, out_channels=62, kernel_size=3) #input:5X5X30, out:3X3X62, kernel:3X3, RF: 32      \r\n",
        "        #self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=3) #input:3X3X62, out:1X1X62, kernel:3X3, stride=3, RF: 32  \r\n",
        "\r\n",
        "\r\n",
        "        # use the following block for 'digits' split\r\n",
        "        self.conv6 = nn.Conv2d(in_channels=30, out_channels=10, kernel_size=3) #input:5X5X30, out:3X3X62, kernel:3X3, RF: 32      \r\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=3) #input:3X3X62, out:1X1X62, kernel:3X3, stride=3, RF: 32          \r\n",
        "\r\n",
        "    def forward(self, t):\r\n",
        "        # (1) input layer\r\n",
        "        t = t\r\n",
        "\r\n",
        "        # (2) hidden conv layer\r\n",
        "        t = self.conv1(t)\r\n",
        "        t = F.relu(t)      \r\n",
        "\r\n",
        "        # (3) hidden conv layer\r\n",
        "        t = self.conv2(t)\r\n",
        "        t = F.relu(t)\r\n",
        "\r\n",
        "        # (4) hidden max_pool layer\r\n",
        "        t = self.pool1(t)\r\n",
        "\r\n",
        "        # (5) hidden conv layer\r\n",
        "        t = self.conv3(t)\r\n",
        "        t = F.relu(t)\r\n",
        "\r\n",
        "        # (6) hidden conv layer\r\n",
        "        t = self.conv4(t)\r\n",
        "        t = F.relu(t)\r\n",
        "\r\n",
        "        # (7) hidden max_pool layer\r\n",
        "        t = self.pool2(t)\r\n",
        "\r\n",
        "        # (8) hidden conv layer\r\n",
        "        t = self.conv5(t)\r\n",
        "        t = F.relu(t)\r\n",
        "\r\n",
        "        # (9) hidden conv layer\r\n",
        "        t = self.conv6(t)        \r\n",
        "        # do not use ReLU here\r\n",
        "\r\n",
        "        # (10) hidden conv layer       \r\n",
        "        t = self.avg_pool(t)        \r\n",
        "\r\n",
        "        #t = t.view(-1, 62)      #flatten for split by class\r\n",
        "        t = t.view(-1, 10)     #use for split by 'digits'      \r\n",
        "        t = F.log_softmax(t, dim=1)#output with log softmax\r\n",
        "\r\n",
        "        return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihjXGspxORy"
      },
      "source": [
        "# Train Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDIQBacGie9O",
        "outputId": "3e1964d7-a607-4530-8f43-704e523ba596"
      },
      "source": [
        "torch.set_grad_enabled(True) #need calculate gradients\n",
        "\n",
        "# Make sure trains in GPU only\n",
        "network = Emnist_Network().to(device)\n",
        "\n",
        "#load the training data as batches of 100\n",
        "emnist_train_loader = torch.utils.data.DataLoader(\n",
        "    emnist_train_set\n",
        "    ,batch_size=100\n",
        "    ,shuffle = True)\n",
        "\n",
        "# debug pupose\n",
        "print(network)\n",
        "\n",
        "#select optmizer\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(20):   #limit to 20 epochs\n",
        "    total_loss = 0    \n",
        "    total_correct = 0\n",
        "    #counter=0 #used for debug only\n",
        "\n",
        "    for batch in emnist_train_loader: # Get Batch\n",
        "        #if(counter > 100): #for debug purpose only to see if network is learning\n",
        "        #  break\n",
        "        #counter+=1      \n",
        "        images, labels = batch        \n",
        "        images, labels=images.to(device), labels.to(device)    # input and labels need to be sent to GPU  \n",
        "           \n",
        "        preds = network(images) # Pass Batch        \n",
        "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
        "\n",
        "        optimizer.zero_grad() # reset gradients for each batch\n",
        "        loss.backward() # Calculate Gradients\n",
        "        optimizer.step() # Update Weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_num_correct(preds, labels)     \n",
        "\n",
        "    print(\n",
        "        \"epoch\", epoch, \n",
        "        \"total_correct: % of correct\", total_correct, (total_correct/dataset_size)*100,\n",
        "        \"loss:\", total_loss\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emnist_Network(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv5): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv6): Conv2d(30, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (avg_pool): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
            ")\n",
            "epoch 0 total_correct: % of correct 232170 96.7375 loss: 251.03245255339425\n",
            "epoch 1 total_correct: % of correct 236772 98.655 loss: 110.62335897196317\n",
            "epoch 2 total_correct: % of correct 236905 98.71041666666667 loss: 105.1296642308298\n",
            "epoch 3 total_correct: % of correct 237098 98.79083333333334 loss: 98.61464099913428\n",
            "epoch 4 total_correct: % of correct 237173 98.82208333333334 loss: 96.75471425702563\n",
            "epoch 5 total_correct: % of correct 237245 98.85208333333333 loss: 95.25903864811698\n",
            "epoch 6 total_correct: % of correct 237256 98.85666666666667 loss: 94.60490418042173\n",
            "epoch 7 total_correct: % of correct 237297 98.87375 loss: 95.16098748416698\n",
            "epoch 8 total_correct: % of correct 237245 98.85208333333333 loss: 94.59248302309425\n",
            "epoch 9 total_correct: % of correct 237294 98.8725 loss: 94.53093029993761\n",
            "epoch 10 total_correct: % of correct 237293 98.87208333333334 loss: 94.64431773910474\n",
            "epoch 11 total_correct: % of correct 237361 98.90041666666667 loss: 92.92297828909068\n",
            "epoch 12 total_correct: % of correct 237405 98.91875 loss: 90.42038390174275\n",
            "epoch 13 total_correct: % of correct 237303 98.87625 loss: 92.79333130324085\n",
            "epoch 14 total_correct: % of correct 237412 98.92166666666667 loss: 89.82929859829892\n",
            "epoch 15 total_correct: % of correct 237279 98.86625 loss: 96.01349184424907\n",
            "epoch 16 total_correct: % of correct 237426 98.9275 loss: 90.2240623217076\n",
            "epoch 17 total_correct: % of correct 237436 98.93166666666666 loss: 89.21218147259788\n",
            "epoch 18 total_correct: % of correct 237405 98.91875 loss: 93.62948273013899\n",
            "epoch 19 total_correct: % of correct 237531 98.97125 loss: 86.21761144706397\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}