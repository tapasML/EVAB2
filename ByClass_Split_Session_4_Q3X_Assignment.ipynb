{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ByClass_Split_Session_4_Q3X_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjIKPTx83K9VjbuYI3nxLc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "739e97aa1c814752bc2f4d54cded7b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_63979105821d4256b745cca7f1e62c8b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6d81078f4604207bc5570054b5edb35",
              "IPY_MODEL_ec9ccde3c1b549de86124074ebd835e3"
            ]
          }
        },
        "63979105821d4256b745cca7f1e62c8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6d81078f4604207bc5570054b5edb35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e6237a9142ca4ecfad0b362583fdc196",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9f4e52e37944666957cb964cd32ed48"
          }
        },
        "ec9ccde3c1b549de86124074ebd835e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eabb64391fc049e48c0acb291e34f82b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 561758208/? [00:30&lt;00:00, 33786175.44it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8dc275fcfa9e479798d48d0d4ea1c5c5"
          }
        },
        "e6237a9142ca4ecfad0b362583fdc196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9f4e52e37944666957cb964cd32ed48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eabb64391fc049e48c0acb291e34f82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8dc275fcfa9e479798d48d0d4ea1c5c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tapasML/EVAB2/blob/main/ByClass_Split_Session_4_Q3X_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtbLBAupvEvL"
      },
      "source": [
        "#                                                 **This is EMNIST**\r\n",
        "\r\n",
        "#   This notebook model uses split 'ByClass'\r\n",
        "# Split 'byclass' is **unbalanced** (training accuracy lesser) \r\n",
        "# compared to digits/ MNIST/ letters (**balanced** train set)\r\n",
        "\r\n",
        "#Note: The results between 'byClass' and 'digits' vary a lot.\r\n",
        "#ByClass is unbalanced (unequal distribution of labels)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308,
          "referenced_widgets": [
            "739e97aa1c814752bc2f4d54cded7b2d",
            "63979105821d4256b745cca7f1e62c8b",
            "d6d81078f4604207bc5570054b5edb35",
            "ec9ccde3c1b549de86124074ebd835e3",
            "e6237a9142ca4ecfad0b362583fdc196",
            "a9f4e52e37944666957cb964cd32ed48",
            "eabb64391fc049e48c0acb291e34f82b",
            "8dc275fcfa9e479798d48d0d4ea1c5c5"
          ]
        },
        "id": "PRf-spLDWGzC",
        "outputId": "8811baf5-8749-45c2-85f8-1d9f5b8c81e4"
      },
      "source": [
        "#import torch, tochvision packages \r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim \r\n",
        "import torchvision \r\n",
        "import torchvision.transforms as transforms  \r\n",
        "\r\n",
        "#define device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "#download the dataset in /data_emnist folder as Tensors\r\n",
        "emnist_train_set = torchvision.datasets.EMNIST(\r\n",
        "    root='./data_emnist'\r\n",
        "    ,train=True\r\n",
        "    ,split='byclass' #split type =byclass\r\n",
        "    #,split='digits' #split type =digits\r\n",
        "    ,download=True\r\n",
        "    ,transform=transforms.Compose([\r\n",
        "        # if split 'byclass' vertical 90 degree + horizontal flip image to dispay properly.\r\n",
        "        #lambda img: torchvision.transforms.functional.rotate(img, -90), \r\n",
        "        #lambda img: torchvision.transforms.functional.hflip(img),\r\n",
        "        transforms.ToTensor()\r\n",
        "    ])\r\n",
        ")\r\n",
        "dataset_size= len(emnist_train_set)\r\n",
        "print('dataset size = ', dataset_size) #around 700K images for byClass dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting zip archive\n",
            "Downloading http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to ./data_emnist/EMNIST/raw/emnist.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "739e97aa1c814752bc2f4d54cded7b2d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data_emnist/EMNIST/raw/emnist.zip to ./data_emnist/EMNIST/raw\n",
            "Processing byclass\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processing bymerge\n",
            "Processing balanced\n",
            "Processing letters\n",
            "Processing digits\n",
            "Processing mnist\n",
            "Done!\n",
            "dataset size =  697932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmS794WIv3x6"
      },
      "source": [
        "# **Build the Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "688V-MZIw2Cx"
      },
      "source": [
        "#@disclaimer: get_num_correct() function is example of plagiarism\r\n",
        "\r\n",
        "#utility method to count how many predictions match label.\r\n",
        "def get_num_correct(preds, labels):\r\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()\r\n",
        "\r\n",
        "#define the network\r\n",
        "class Emnist_Network(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__() #initialize parent       \r\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,  out_channels=10, kernel_size=3, padding=1)#input:28X28X1,  out:28X28X10, kernel:3X3, RF: 3\r\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, padding=1)#input:28X28X10, out:28X28X10, kernel:3X3, RF: 5\r\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  #input:28X28X10, out:14X14X10, kernel:2X2, stride=2, RF: 10\r\n",
        "\r\n",
        "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1)#input:14X14X10, out:14X14X20, kernel:3X3, RF: 12\r\n",
        "        self.conv4 = nn.Conv2d(in_channels=20, out_channels=20, kernel_size=3, padding=1)#input:14X14X20, out:14X14X20, kernel:3X3, RF: 14\r\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  #input:14X14X20, out:7X7X20, kernel:2X2, stride=2, RF: 28\r\n",
        "\r\n",
        "        self.conv5 = nn.Conv2d(in_channels=20, out_channels=30, kernel_size=3) #input:7X7X20, out:5X5X30, kernel:3X3, RF: 30\r\n",
        "\r\n",
        "        # for split by class\r\n",
        "        self.conv6 = nn.Conv2d(in_channels=30, out_channels=62, kernel_size=3) #input:5X5X30, out:3X3X62, kernel:3X3, RF: 32      \r\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=3) #input:3X3X62, out:1X1X62, kernel:3X3, stride=3, RF: 32  \r\n",
        "\r\n",
        "\r\n",
        "        # use the following block for 'digits' split\r\n",
        "        #self.conv6 = nn.Conv2d(in_channels=30, out_channels=10, kernel_size=3) #input:5X5X30, out:3X3X62, kernel:3X3, RF: 32      \r\n",
        "        #self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=3) #input:3X3X62, out:1X1X62, kernel:3X3, stride=3, RF: 32          \r\n",
        "\r\n",
        "    def forward(self, t):\r\n",
        "        # (1) input layer\r\n",
        "        t = t\r\n",
        "\r\n",
        "        # (2) hidden conv layer\r\n",
        "        t = self.conv1(t)\r\n",
        "        t = F.relu(t)      \r\n",
        "\r\n",
        "        # (3) hidden conv layer\r\n",
        "        t = self.conv2(t)\r\n",
        "        t = F.relu(t)\r\n",
        "\r\n",
        "        # (4) hidden max_pool layer\r\n",
        "        t = self.pool1(t)\r\n",
        "\r\n",
        "        # (5) hidden conv layer\r\n",
        "        t = self.conv3(t)\r\n",
        "        t = F.relu(t)\r\n",
        "\r\n",
        "        # (6) hidden conv layer\r\n",
        "        t = self.conv4(t)\r\n",
        "        t = F.relu(t)\r\n",
        "\r\n",
        "        # (7) hidden max_pool layer\r\n",
        "        t = self.pool2(t)\r\n",
        "\r\n",
        "        # (8) hidden conv layer\r\n",
        "        t = self.conv5(t)\r\n",
        "        t = F.relu(t)\r\n",
        "\r\n",
        "        # (9) hidden conv layer\r\n",
        "        t = self.conv6(t)        \r\n",
        "        # do not use ReLU here\r\n",
        "\r\n",
        "        # (10) hidden conv layer       \r\n",
        "        t = self.avg_pool(t)        \r\n",
        "\r\n",
        "        t = t.view(-1, 62)      #flatten for split by class\r\n",
        "        #t = t.view(-1, 10)     #use for split by 'digits'      \r\n",
        "        t = F.log_softmax(t, dim=1)#output with log softmax\r\n",
        "\r\n",
        "        return t"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihjXGspxORy"
      },
      "source": [
        "# Train Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDIQBacGie9O",
        "outputId": "f4724199-ec14-4419-9029-b01f7faf3c6d"
      },
      "source": [
        "torch.set_grad_enabled(True) #need calculate gradients\n",
        "\n",
        "# Make sure trains in GPU only\n",
        "network = Emnist_Network().to(device)\n",
        "\n",
        "#load the training data as batches of 100\n",
        "emnist_train_loader = torch.utils.data.DataLoader(\n",
        "    emnist_train_set\n",
        "    ,batch_size=100\n",
        "    ,shuffle = True)\n",
        "\n",
        "# debug pupose\n",
        "print(network)\n",
        "\n",
        "#select optmizer\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(20):   #limit to 20 epochs\n",
        "    total_loss = 0    \n",
        "    total_correct = 0\n",
        "    #counter=0 #used for debug only\n",
        "\n",
        "    for batch in emnist_train_loader: # Get Batch\n",
        "        #if(counter > 100): #for debug purpose only to see if network is learning\n",
        "        #  break\n",
        "        #counter+=1      \n",
        "        images, labels = batch        \n",
        "        images, labels=images.to(device), labels.to(device)    # input and labels need to be sent to GPU  \n",
        "           \n",
        "        preds = network(images) # Pass Batch        \n",
        "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
        "\n",
        "        optimizer.zero_grad() # reset gradients for each batch\n",
        "        loss.backward() # Calculate Gradients\n",
        "        optimizer.step() # Update Weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += get_num_correct(preds, labels)     \n",
        "\n",
        "    print(\n",
        "        \"epoch\", epoch, \n",
        "        \"total_correct ,  % of correct\", total_correct, (total_correct/dataset_size)*100,\n",
        "        \"loss:\", total_loss\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emnist_Network(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv5): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv6): Conv2d(30, 62, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (avg_pool): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}